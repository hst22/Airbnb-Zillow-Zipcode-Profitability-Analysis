---
title: "Airbnb - Zillow Zip code Profitability Analysis"
author: "by Hasnat Tahir"
output:
  html_document:
    df_print: paged
  word_document: default
  always_allow_html: yes
  pdf_document: default
---
##   {.tabset}

### Introduction

**Background**
\ 

A real-estate company wants an analytically backed data product to help them decide which zip codes would generate the most profit on 2-bedroom short term rentals within New York City.

\ 
**About the report:**
This case study contains a detailed analysis of airbnb and zillow data sets. Data analyses have been carried out on listings corresponding to 2 bedroom properties in New York. The end result is a list of zip codes that are the most profitable short-term rentals based on Return on Investment, Annual Revenue, and popularity of the listed properties.  

\ 

#### Data Sets Available:

* Airbnb property listing data set containing information on the listing including location, number of bedrooms, room types (entire home/private home/shared home).
* Zillow 2 – Bedroom Time Series data set containing cost data to determine the average property price for 2 bedrooms.

\ 
Packages
```{r Packages, message=FALSE, warning=FALSE}
packs = c(
  "funModeling", 
  "plotly", 
  "maps", 
  "naniar", 
  "ggmap")

for (pack in packs) {
  if (!(pack %in% installed.packages()[, "Package"])) {
  install.packages(packs)
  }
}
```

Libraries
```{r Libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(DataExplorer)
library(ggplot2)
library(dplyr)
library(funModeling)
library(ggmap)
library(plotly)
library(viridis)
library(hrbrthemes)
```

\ 

**Importing Airbnb and Zillow Datasets**
```{r Data set import}
airbnb = read.csv('D:/University/Airbnb - Zillow Profitability Analysis/Data/listings.csv')

zillow = read.csv('D:/University/Airbnb - Zillow Profitability Analysis/Data/zillow_data.csv', check.names = FALSE)

#Dimensions

dim(airbnb)
dim(zillow)
```
* Airbnb data has 48895 observations and 106 variables.
* Zillow data has 8946 observations and 262 variables.

 *Note: Zillow data set contains data only for 2 bedroom properties*

\ 

**Defining global variables:**
```{r Global Variable}
#Global variable
State_in_analysis = "NY"
nBedrooms = 2
```
Code has been prepared according to the need of the company, can be used for the analysis of states other than NY if more data is added to the data sets, provided the data set is clean and structured. The variables can be changed as per the requirement. In this case, we have to consider New York and 2 bedrooms.

*As per the problem statement, analyses will be carried out on 2 bedroom properties in New York. Data quality needs to be checked based on the aforementioned conditions.*

<a href="#top">Back to top</a>

### Quality Check & Data Cleaning

**Uniformity check of 'state' variable before filtering:**
```{r Uniformity check - State}
table(airbnb$state) #airbnb
table(zillow$State) #zillow

#duplicate values check
any(duplicated(airbnb))
any(duplicated(zillow))
```
* Output shows there are mutiple entries for New York City, like "New York", "NY", "Ny", "ny". However, in zillow data set all the New York observations are properly encoded. We can filter airbnb data set based on number of bedrooms and zillow data set based on State, this will eliminate the need of modifying the non-uniform New York observations in airbnb data set.
* There are no duplicate values in both the data sets.

\ 


**Changing the variable name as 'zipcode' in zillow data for uniformity:**
```{r zipcode in zillow}
colnames(zillow)[2] = "zipcode"
colnames(zillow)[2] #cross-checking
```
Looks good!

\ 

**Modifying state variable for uniformity:**
```{r Modify State variable}
airbnb$state = gsub('New York', 'NY', airbnb$state)
airbnb$state = gsub('ny', 'NY', airbnb$state)
airbnb$state = gsub('Ny', 'NY', airbnb$state)
airbnb$state = gsub('NY ', 'NY', airbnb$state) # had one space after NY

#cross-checking for unique entries 
table(airbnb$state)
```
Looks good! Now, all the observations in the ‘state’ variable corresponding to New York are encoded as ‘NY’.

\ 

**Checking for abnormal zip codes:**
```{r Abnormal values}
#airbnb 
sum(airbnb$zipcode== "" & airbnb$state == "NY" & airbnb$bedrooms == 2, na.rm = T) #missing values

dim(airbnb[nchar(as.character(airbnb$zipcode)) != 5 & nchar(as.character(airbnb$zipcode)) !=0,])[1]  # abnormal zip codes in airbnb

unique(nchar(as.character(airbnb$zipcode))) # digit length of zip codes in airbnb

#zillow
sum(zillow$zipcode == "" & zillow$State == "NY") # missing values

dim(zillow[nchar(as.character(zillow$zipcode)) != 5,])[1] # abnormal zip codes in zillow 

unique(nchar(zillow$zipcode)) # digit length of zip codes in zillow

```
* There are 50 missing observations in zipcode variable of airbnb data set that is necessary for this analysis(cosidering NY and 2 bedrooms).
* Also, there are some zip codes having character length not equal to 5 in both the data sets. These zip codes will be taken care of after the merge, if there are some left.

**Unique zip codes:**
```{r Unique zip codes}
#in airbnb for 2-bedroom NY properties 
length(unique(airbnb[airbnb$state=="NY" & airbnb$bedrooms==2,]$zipcode))

#in zillow for NY properties 
length(unique(as.character(zillow[zillow$State== "NY",]$zipcode)))

```
* airbnb data set has 171 unique zip codes for 2-bedroom NY properties
* zillow data set has 475 unique zip codes for 2-bedroom NY properties

\ 

**Filter Merge function:** 
```{r Filter and merge}

fil_merge = function(listing_data, cost_data, columnname){
  data_1 = filter(listing_data, listing_data$bedrooms == nBedrooms) #filter
  data_2 = filter(cost_data, cost_data$State == State_in_analysis)
  merged_data = merge(data_1, data_2, by = columnname) #merge
  return(merged_data)
}

```

* The function 'fil_merge' will first filter on the basis of 'state_in_analysis' and 'nBedrooms' provided in global variable chunk and then merge the data sets together on a specified column.
* This funtion can be used to filter and merge on the basis of different criteria as per the need.

\ 

**Merging airbnb and zillow:**
```{r Merging airbnb and zillow}
data_merged = fil_merge(airbnb, zillow, "zipcode") # merging airbnb and zillow data set 

dim(data_merged)

length(unique(data_merged$zipcode)) #unique zip codes
```
 
* Using the fil_merge function, airbnb and zillow data have been filtered and merged for NY and 2 bedrooms.
* Output data set contains 1566 observations and 367 variables.

\ 

**Variable selection:**
```{r Variable Selection}
#subseting a new data frame for further analyses
main_data = subset(data_merged, select = c('id','host_id','neighbourhood_cleansed',
                                     'neighbourhood_group_cleansed','zipcode','latitude',
                                     'longitude','is_location_exact','property_type',
                                     'room_type','bathrooms','beds','square_feet','price',
                                     'weekly_price','monthly_price','cleaning_fee',
                                     'minimum_nights','maximum_nights',
                                     'availability_365','review_scores_rating',
                                     'reviews_per_month','State', '2017-06'))

dim(main_data)
colnames(main_data)

unique(nchar(as.character(main_data$zipcode))) #check for abnormal zip codes
```

* Total 24 variables have been seleted based on their usability.
  + "id" and "host_id" will provides uniqueness that can be used to filter out duplicates.
  + "neighbourhood_cleansed", "latitude", etc to retain location information. 
  + "property_type", "bedrooms", etc  to get information about the properties.
  + "price", "cleaning_fee", "2017-06", etc for revenue and other calculations.
  + "review_scores_rating" and "reviews_per_month" for analyzing the quality and popularity of the properties.
* Data set('main_data') has been created for analyses that contains 1566 observations and 24 variables.
* Data set has 11 categorical variables and 13 quantitative variables.
* Also, no abnormal zip code is present in the merged data set.


\ 

**Checking for missing values:**
```{r Missing Values}
plot_missing(main_data, title = '% missing values in main_data', geom_label_args = list("size" = 3, "label.padding" = unit(0.1, "lines")))
```

* From the above plot:
  + Band categorization based on % of missing values- Red/Good when <= 5%, Green/OK when <= 40%, and Blue/Remove >= 80%. 
  + Variables "weekly_price", "monthly_price", and "square_feet" cannot be used for analysis, as majority of the values are missing.
  + Variables "cleaning_fee", "reviews_per_month", "review_score_rating", and "bathrooms" have less than 25% of the values missing. Imputing values in these variables can be considered.

\ 
* As per the question, we have to consider Short-Term rentals which is between 30-180 days.

**Checking for values above 180 days of minimum stay:**
```{r Abnormal values_2}
check = main_data$minimum_nights[main_data$minimum_nights>180]
check
```
There are three abnormal observations

\ 

**Removing the above 3 observations**
```{r Remove abnormals}
dim(main_data)

main_data1 = filter(main_data, minimum_nights != 265 & minimum_nights != 365)

dim(main_data1) #cross-checking

```
The abnormal obeservations have been removed. 
\ 


#### **Data Quality Insights:**
* There are 50 missing values in the zipcode variable of airbnb data set corresponding to 2 bedroom properties in New York. These observations are crucial for our analysis. These observations can be imputed using the latitude and longitude variables and reverse geocoding from an [API](https://github.com/mhudecheck/revgeo/).
* Lack of uniformity in the state variable of airbnb data set was observed. Observations for "New York" were recorded under multiple entries like "NY", 'Ny", "ny", etc. This could lead to the loss of some data during analysis if not checked properly. 
* Important variables "weekly_price", "monthly_price", and "sq_feet" have most of the observations missing making them redundant for analysis. Otherwise, these variables could be used in cost-related calculations and analyses.
* Airbnb data set has 171 unique zip codes for 2-bedroom New York properties and zillow data set has 475 unique zip codes. After combining both the data sets only 25 unique zip codes are left for analyses making the main_data thin. This is a drastic reduction in data quality. 

<a href="#top">Back to top</a>

### EDA

**Unique values:**
```{r Frequency}
#Frequency of observations
freq(data = main_data1, input = c('zipcode', 'property_type', 'room_type', 'bathrooms', 'beds'))
```

* Based on the tables and plots above following can be stated:
  + zip codes 11215, 10036, and 10003 contain approximately one-third of the properties listed in the data set.
  + Apartment is the most available property type.
  + Listed properties have number of bathrooms ranging from 0 to 3.5; however, 1 bathroom is most common.
  + Listed properties have number of beds ranging from 0 to 6; however, 2 beds are most common.

\ 

**Customized Histogram Function:**
```{r Histogram function}
hist_plot = function(dataset,cname,variable, width){
                  ggplot(data=dataset, aes(x=cname)) + geom_histogram(aes(y=..density..), 
                  color="darkblue", fill="lightblue", binwidth = width) + 
                  geom_vline(xintercept = median(cname, na.rm = T), 
                             color = 'Red', linetype = "dashed") +
                  geom_vline(xintercept = mean(cname, na.rm = T), color = 'Black', 
                             linetype = "dashed")+ 
                  scale_y_continuous(name= "Frequency", labels = scales::comma) +
                  scale_x_continuous(name= variable, labels = scales::comma) +
                  ggtitle(paste("Histogram of", variable )) + 
                  labs(caption = "Black dotted  line: Mean   |    Red dotted line: Median") +
                  geom_density()
}
```
The above histogram function takes in data set, column name, variable name, and binwidth as input. 

**Histogram Plots**
```{r Histograms, warning=FALSE}
hist_plot(main_data1, main_data1$price,"Price", 30)
hist_plot(main_data1, main_data1$cleaning_fee,"Cleaning Fee", 20)
hist_plot(main_data1, main_data1$review_scores_rating,"Ratings", 1)
hist_plot(main_data1, main_data1$reviews_per_month,"Reviews per month", 0.2)
hist_plot(main_data1, main_data1$beds,"Number of beds", 0.5)
hist_plot(main_data1, main_data1$bathrooms,"Number of bathrooms", 0.5)
hist_plot(main_data1, main_data1$`2017-06`,"Median Price in Area", 100000)
```

**Key Observations so far:**

  + Variables square_feet, monthly_price, and weekly_price cannot be used as they contain too many NA values.
  + NA values in variables cleaning_fee, bathrooms, review_score_ratings, and reviews_per_month can be imputed for further analyses.
  + Variable price is the charge per night whereas the variable '2017-06' i.e. property cost is the median cost. This must be kept in mind at the time of calculations.
  + Right skewed variables = (Price, cleaning_fee, reviews_per_month, beds, bathrooms, 2017-06)
  + Left skewed variables = (review_scores_ratings)

*Outlier treatment has been skipped, as it is not required in the case at hand. Removing outliers in this case might lead to loosing some important observations. e.g. a property that has a very high annual revenue can be an outlier, but we cannot remove it.*

<a href="#top">Back to top</a>

### Data Munging

**Imputation of NA values:**
```{r Imputation}
#for review score rating
main_data1$review_scores_rating[is.na(main_data1$review_scores_rating)] = median(main_data1$review_scores_rating, na.rm =TRUE)  

#for review per month
main_data1$reviews_per_month[is.na(main_data1$reviews_per_month)] = median(main_data1$reviews_per_month, na.rm =TRUE)  

#for bathrooms
main_data1$bathrooms[is.na(main_data1$bathrooms)] = median(main_data1$bathrooms, na.rm =TRUE)  

#for cleaning fee; as sometimes it's included in the price
main_data1$cleaning_fee[is.na(main_data1$cleaning_fee)] = 0  

#Cross-check of missing values
sum(is.na(main_data1$review_scores_rating))
sum(is.na(main_data1$reviews_per_month))
sum(is.na(main_data1$bathrooms))
sum(is.na(main_data1$cleaning_fee))
```
* As variables review_score_rating, review_per_month, and bathrooms have skewed distributions, NA values have been imputed with the corresponding median values.
* In variable cleaning_fee NA values have been replaced by 0, as in most of the cases cleaning fee is included in the rent of the property.

\ 

#### Cleaning fee recovered as revenue:
* Usually, the cleaning fee charged by the host is not completely spent in cleaning the property. Some percentage of it gets converted as revenue. As a matter of fact, hosts charge a slightly high cleaning fee in order to be on the safer side. However, the amount that is utilized as the cleaning fee varies from property to property.
* In the available data, in order to take account of this variation, I've considered that the cleaning fee varies on the basis of size of the property(sq_feet), number of bathrooms(bathrooms), and number of beds(beds) in the property. But, I can only use beds and bathrooms variables as sq_feet variable has too many NA values.
* Now, since, we are dealing with only 2 bedroom properties, we can use this as a fixed measure and categorize the cleaning fee.
* Properties having more than 2 bathrooms or having more than 4 beds will be categorized as 'high cleaning fee' properties and remaining as 'low cleaning fee' properties.
* **(Assumption)** 'High cleaning fee' properties recover 15 percent of the charged cleaning fee as revenue and 'Low cleaning fee' properties recover 30 percent of the charged cleaning fee. 

\ 

**New variable containing cleaning fee categories:**
```{r Cleaning fee category}
#new column for category wise cleaning fee 
main_data1$cf_cat = ifelse(main_data1$beds>=4 | main_data1$bathrooms > 2, 0.15, 0.3)

table(main_data1$cf_cat) # checking the fequency of categories
```
* A new column 'cf_cat' has been created for cleaning fee categories to be used in further calculations.
* Aprroximately 90 percent of the data has been categorized as low cleaning fee property.

\ 

**Revenue per annum:**
Annual revenue will be calculated considering 75% occupancy and percentage of cleaning fee recovered as revenue(explained above).
```{r Revenue per annum}
main_data1$revenue_perannum = (main_data1$price*365*0.75) + (main_data1$cleaning_fee*main_data1$cf_cat*365*0.75)

summary(main_data1$revenue_perannum)
```
A new variable 'revenue_perannum' was created containing annual revenue of individual properties.  

\ 

**Return on investment:**
ROI calculation using revenue_perannum and cost of individual properties.
```{r ROI}
main_data1$roi = main_data1$revenue_perannum/main_data1$`2017-06`*100

#rounding to 0 decimal place
main_data1$roi = round(main_data1$roi,2)

summary(main_data1$roi)
```
*Latest available median price within the zip code, i.e. '2017-06' variable has been assumed as cost of individual properties.*

<a href="#top">Back to top</a>

### Visual Data Narrative

**Quadrant Analysis plot to filter most profitable properties:**
```{r Quad Analysis Plot}
#plot
q_p = main_data1 %>%
mutate(text = paste("Zipcode: ", main_data1$zipcode,"\nNeighborhood :",
                    main_data1$neighbourhood_cleansed, "\nMedian property cost: ",
                    main_data1$`2017-06`, "\nRevenue: ", main_data1$revenue_perannum,
                    "\nROI: ", main_data1$roi, "\nID: ", main_data1$id, sep="")) %>%
  
ggplot( aes(x=revenue_perannum, y=roi, color = zipcode, text=text)) +
  geom_point(alpha=0.7) + geom_hline(yintercept = median(main_data1$roi))+
  geom_vline(xintercept = median(main_data1$revenue_perannum))+
  scale_size(range = c(1.4, 19)) + scale_color_viridis(discrete=TRUE, guide=FALSE) +
  theme_ipsum()+ ggtitle("Quadrant Analysis: Revenue vs ROI") + 
  ylab("Return on investment") +
  scale_x_continuous(name="Revenue per annum", labels = scales::comma)

qp = ggplotly(q_p, tooltip="text")
qp
 
```

##### About the above plot:
* Scatter plot of Revenue per annum vs Return on investment for all properties
* The whole plot has been divided into four quadrants by lines of respective median values of axis parameters
* Tool-tip has details about the zip code, neighbourhood, Median property cost, revenue, ROI, and Property ID.(Please, hover over the scatter points)
* Scatter points have been color coded based on zip codes
* A section of the plot can be selected in order to further zoom-in
* Multiple data points can be visualized and compared after selecting the "compare data on hover" feature on the top right corner of the plot
* zip codes in the legend can be selected for a detailed analysis of a particular zip code. 
    
\ 

##### **Quadrant Analysis:**
* In order to ensure profitability, both revenue generated per annum and ROI are important.
* A property having high revenue per annum won't be a great investment if its ROI is low.
* In order to ensure that the real estate company end up with a property that has a good revenue as well as having a comparatively good ROI than most of the properties, Quadrant Analysis has been performed.
* (Key Observation) From the above plot, the properties having higher annual revenue and higher ROI than 50% of the available properties lie in the top right quadrant.

\ 

**Filtering based on Quadrant analysis:**
```{r Filtering after QA}

#filtering the bottom right quadrant data
final_data = subset(main_data1, roi > median(roi) & revenue_perannum > median(revenue_perannum))

dim(main_data1)
dim(final_data)

# frequency of zip codes
freq(data = final_data, input = c('zipcode'))
```
* After the filter, we have 552 properties left that are comparatively more profitable.
* zip codes 10036, 10025, and 10011 contains more than one-third of the filtered properties.
* zip code 10305 has only one property that got filtered.
* Now, there are only 16 zip codes left for further analysis.

\ 

**Further analysis:**
```{r Group by}
#grouping by zipcode, and ordering by Median revenue

#median review per month variable included as a measurement for popularity

by_zip1 = final_data %>% group_by(zipcode)

final_group = by_zip1 %>% summarise(Median_ROI = median(roi), 
                                    Median_rpm= median(reviews_per_month), 
                                    Median_revenue = median(revenue_perannum))

final_group2 = final_group[order(-final_group$Median_ROI, 
                                 -final_group$Median_rpm, 
                                 -final_group$Median_revenue),]

final_group2
```

* The table above has been grouped on zipcode. 
* review_per_month has been considered as a measure of popularity; higher number of reviews to some extent corresponds to relatively higher number of bookings.
* Above table has been ordered first on the basis of descending ROI then on descending reviews_per_month and lastly on annual revenue.    

\ 

#### zip codes selection on the basis of the above table:

* Order of priority of variables for profitability considered as, Return on investment> Popularity(reviews_per_month)> Annual Revenue
* zip code 10305 appears to have the highest ROI among the filtered. However, it can be observed from the frequency table, only one property is left after quadrant analysis filtering. This zip code cannot be selected because of the aforementioned reason.
* From the above table, top 5 zip codes excluding 10305 has been selected as the most profitable ones for Short term rentals of 2 bedroom properties in New York.

**Selected zip codes = (11215, 10025, 10036, 10003, and 11231)**

\ 

**Neighbourhood Analysis:**

Drill down of selected zip codes to analyse the neighbourhoods  
```{r Neighbourhoods, message=FALSE}
#subsetting the selected zip codes
data_nbr = final_data[final_data$zipcode %in% c("11215", "10025", "10036", "10003", "11231"),]

#frequency plot
freq(data = data_nbr, input = c("zipcode","neighbourhood_group_cleansed",
                                "neighbourhood_cleansed"))

```

* 10036 has the most number of properties that have been filtered for being most profitable.
* Manhattan and Brooklyn are the most profitable boroughs in New York. With Manhattan having most of the total filtered properties.
* Neighbourhood wise, Hell's Kitchen and Upper West side contains  more than half of the profitable properties(~54%).

**Jitter Plot: ROI in Neighbourhoods**
```{r Jitter plot, message=FALSE, warning=FALSE}
#Jitter plot
J_P = ggplot(data_nbr,aes(x=zipcode, y=roi, colour=neighbourhood_cleansed)) +
      geom_jitter(aes(text = paste("\nID: ", data_nbr$id, "\nMedian property cost: ",
                               data_nbr$`2017-06`, "\nRevenue: ", data_nbr$revenue_perannum,
                               "\nReviews/month: ", data_nbr$reviews_per_month,
                               sep="")), width=0.25, alpha=0.8,)+
      labs(title = "ROI in Neighbourhoods", x = "Zipcode", y = "Return on Investment") + 
      stat_summary(fun.y=median, geom="point", shape=18, size=1, color="Black")

JP = ggplotly(J_P)
JP
```

About the above plot:

* Jitter plot of zip codes vs Return on investment for all properties
* Black diamond represents median ROI in that zip code
* Tool-tip has details about the Property ID, Median property cost, Annual revenue, zip code, ROI, Neighbourhood, and Reviews per month.(Please, hover over the Jitter points)
* Jitter points have been color coded based on neighbourhoods
* A section of the plot can be selected in order to further zoom-in
* Multiple data points can be visualized and compared after selecting the "compare data on hover" feature on the top right corner of the plot
* Neighbourhoods in the legend can be selected for a detailed analysis of a particular zip code.


**The Jitter plot provides great visualization for neighbourhood analysis under zip codes up to property ID level. Insights from the Jitter plot:** 

* It can be deduced from the plot that property with ID 2281142 in East Village under the zip code 10003 has the maximum ROI.
* Upper West Side under 10025 too has some properties having ROI more than 25%.
* zip code 11215 is the most popular(based on the assumption of more reviews per month) among selected zip codes, comparatively ROI wise nothing extraordinary can be observed in the plot.

There is always a trade-off when it comes to investment, the real estate company can use the above jitter plot when deciding on buying a property.  


**Mapping Properties**
```{r Map, message=FALSE, warning=FALSE}
#map
mbox = make_bbox(lon = main_data1$longitude, lat = main_data1$latitude, f = 0.01)

sz_map = get_map(location = mbox, maptype = "watercolor", source = "google")

ggmap(sz_map) + geom_point(data = data_nbr, mapping = aes(x = data_nbr$longitude, 
                            y = data_nbr$latitude), color = data_nbr$zipcode, size = 1, 
                            alpha = 0.8) + 
                xlab("longitude") + 
                ylab("latitude") +
                ggtitle("Map-view of selected zipcodes")
```

Map of New York containing the properties under selected zip codes.

<a href="#top">Back to top</a>

### Conclusion

#### Summary:
* Based on all the analyses performed above, it is recommended the real estate company to invest in properties under the zip codes 11215, 10025, 10036, 10003, and 11231.
* The selection was done considering Return on Investment, Annual Revenue, and Popularity, with ROI as the top priority parameter. 
* Manhattan has been found to be the most profitable borough having the majority of the filtered properties. 
* The Jitter plot in the report can be of great help when deciding which neighbourhood to invest in.   

\ 

#### What's Next:
* I would impute the 50 missing zip code observations mentioned earlier in the report using reverse geocoding in order to retain crucial data. This data might have changed the conclusion of the report, though not by much.
* If the number of bookings data for individual properties is collected, the assumption of 75% occupancy can be taken out resulting in better cost calculations. Also, can be used in the analysis to get the popularity of the properties which is one of the key factors when deciding profitability. Also, could be used to find out what kind of property gets booked the most, e.g, what do people prefer, a loft or a villa.
* As the zillow data set has median cost only up to June'17, I would try to get the latest available data and then do the analysis. 
* Cleaning fee analysis can be taken further by studying the "amenities" variable, keywords like "kitchen", "extra pillows and blankets", etc can be fetched out and then categorized under high cleaning fee properties. Similarly, other keywords like "saops", "towels", etc can be fetched out to add these as expenses in revenue calculation.
* If the real estate company is planning to aquire these particular properties listed in the airbnb data set, "availability_365" variable can be used to assess the property-wise profitability.


<a href="#top">Back to top</a>
